{
  "hash": "b5a48bc6fbd1b85562f1c49f1c183667",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Modeling\"\nauthor: \"Holly Probasco\"\nformat: html\noutput: html_document\neditor: visual\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlapply(c(\"readr\",\"tidymodels\", \"caret\", \"tree\", \"rpart\",\"ranger\"), library, character.only = TRUE)\n```\n:::\n\n\n## Introduction\n\nThis modeling file will attempt to look at the different variables in the Diabetes dataset and find the best combination of these variables for a predictive model. We will use 3 different modeling methods: Logistic Regression, Clsssification Tree, and Random Forest. Then, after finding the best model for each type, we will compare those to find the best performing model overall.\n\nThe ultimate goal of modeling in this context is to take existing data in order to predict which factors may result in diabetes for people. In this way, we aim to prevent other people with similar traits from getting the disease. Through this process of model selection, we can get one step closer to a goal like this.\n\n## Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes_data_LR <- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\") |> drop_na() |> mutate(BP_lvl = factor(if_else(HighBP == 0, \"no\", \"high\")), Chol_lvl = factor(if_else(HighChol == 0, \"no\", \"high\")), HeartDA = factor(if_else(HeartDiseaseorAttack == 0, \"no\", \"yes\")), Sex = factor(if_else(Sex == 0, \"F\", \"M\")), Diabetes = factor(if_else(Diabetes_binary == 0, \"no\", \"yes\")), Smoking_status = factor(if_else(Smoker == 0, \"no\", \"yes\")), Health_level = case_when(\n  GenHlth == \"1\" ~ \"excellent\",\n  GenHlth == \"2\" ~ \"verygood\",\n  GenHlth == \"3\" ~ \"good\",\n  GenHlth == \"4\" ~ \"fair\",\n  GenHlth == \"5\" ~ \"poor\",\n  .default = \"NA\"\n)) |> mutate(Health_level = factor(Health_level, levels = c(\"excellent\", \"verygood\", \"good\", \"fair\", \"poor\"),ordered = TRUE)) |> select(Diabetes, BMI, BP_lvl, Chol_lvl, HeartDA, Sex, Smoking_status, Health_level)\n```\n:::\n\n\nSave this to reference later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(diabetes_data_LR, \"diabetes_data.rds\")\n```\n:::\n\n\n## Splitting the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit_data <- initial_split(diabetes_data_LR, prop = .7)\ntrain <- training(split_data)\ntest <- testing(split_data)\n```\n:::\n\n\n## Logistic Regression Models\n\nLogistic regression is a type of model that we use to see if there is a relationship between some predictor variables (x) and a response variable (y). The response variable is binary (like a success/failure), and here we are predicting whether or not someone has Diabetes. We model the probability of success. This is done using a sigmoid curve, which maps all numbers to a value between 0 and 1. Since the curve is then not strictly linear, our slopes model a change in the log-odds of success, rather than just the probability. This way, we do not have to be stuck using a Linear Regression on data that is not continuous!\n\nFitting 3 candidate models\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Recipes setup\n\n# EDA choices of interest by sex\nLR1_rec <- recipe(Diabetes ~ Smoking_status + Health_level + BMI + Sex, data = train) |>\nstep_normalize(BMI) |> step_dummy(Smoking_status, Health_level, Sex)\n\n# Health History choices\nLR2_rec <- recipe(Diabetes ~ BP_lvl + Health_level + Chol_lvl + HeartDA, data = train) |> step_dummy(BP_lvl, Health_level,Chol_lvl, HeartDA)\n\n# My personal guess at optimal choices\nLR3_rec <- recipe(Diabetes ~ HeartDA + Chol_lvl + BMI + BP_lvl, data = train) |> step_normalize(BMI) |> step_dummy(BP_lvl, HeartDA ,Chol_lvl)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set up engine for the logistic regression\nLR_spec <- logistic_reg() |>\nset_engine(\"glm\")\n\n# workflow setup\nLR1_wkf <- workflow() |> add_recipe(LR1_rec) |>\nadd_model(LR_spec)\n\nLR2_wkf <- workflow() |> add_recipe(LR2_rec) |>\nadd_model(LR_spec)\n\nLR3_wkf <- workflow() |> add_recipe(LR3_rec) |>\nadd_model(LR_spec)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create CV folds \ndiab_folds <- vfold_cv(train, 5)\n\n# use CV folds on workflows\nLR1_fit <- LR1_wkf |> fit_resamples(diab_folds, metrics = metric_set( mn_log_loss))\n\nLR2_fit <- LR2_wkf |> fit_resamples(diab_folds, metrics = metric_set( mn_log_loss))\n\nLR3_fit <- LR3_wkf |> fit_resamples(diab_folds, metrics = metric_set(mn_log_loss))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# now we collect the metric we used for each model and compare\nbest_fits <- rbind(LR1_fit |> collect_metrics(),\nLR2_fit |> collect_metrics(),\nLR3_fit |> collect_metrics()) |>\nmutate(Model = c(\"Model1\",\"Model2\",\"Model3\")) |>\nselect(Model, everything())\nbest_fits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 7\n  Model  .metric     .estimator  mean     n  std_err .config             \n  <chr>  <chr>       <chr>      <dbl> <int>    <dbl> <chr>               \n1 Model1 mn_log_loss binary     0.349     5 0.000999 Preprocessor1_Model1\n2 Model2 mn_log_loss binary     0.334     5 0.000830 Preprocessor1_Model1\n3 Model3 mn_log_loss binary     0.344     5 0.00134  Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\nVery similar mean and SE across the 3 models, but it looks like the 2nd model was the best performing as it has the lowest mean log loss.\n\n## Classification Tree\n\nA classification tree model is fit differently than a regression model, in that it uses recursive binary splitting. The goal is to find the optimal cross-entropy for each value of predictor, and split there. Cross-entropy deals with looking at how a split occurs, and measures how \"pure\" split is. This just means how many of one class ended up in the split where the rest of its same class also is. A pure node contains mostly or the entirety of one class. Then nodes are created, and the process continues on each iterative branch. Then, after all the iterative nodes have been created, sometimes pruning is required so that we don't overfit the data. That is, too many nodes is not always ideal, so we can remove some through pruning. We can also choose how many nodes we want to have using Cross Validation.\n\nWe use Classification trees here because the response is categorical, with both numeric and categorical predictors. Also, the built in variable selection is helpful here where we have many variables that could be good predictors, but we want to find the best ones. There are no assumptions like normality/linearity needed, and we don't have to scale the variables like we would for a logistic regression model. They also have a visual element to them that makes the line of thought while model selecting easy to follow.\n\nSet up recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiab_class <- recipe(Diabetes ~ ., data = train) |>\n  step_dummy(all_factor(), -all_outcomes()) |>\n  step_normalize(BMI)\ndiab_class\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:   1\npredictor: 7\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Dummy variables from: all_factor() -all_outcomes()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Centering and scaling for: BMI\n```\n\n\n:::\n:::\n\n\nSet up classification tree model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#we use tune here so that we are about to find the optimal depth and cost complexity through cross validation\ndiab_mod <- decision_tree(tree_depth = tune(),\n        cost_complexity = tune()) |>\n        set_engine(\"rpart\") |>\n        set_mode(\"classification\")\n\n#create workflow\ndiab_wkf <- workflow() |>\n  add_recipe(diab_class) |>\n  add_model(diab_mod)\n\n#now, we find those optimal values for tune using a grid\ndiab_grid <- grid_regular(cost_complexity(),\n                 tree_depth(),\n                 levels = c(10,5))\n\nfits <- diab_wkf |> \n  tune_grid(resamples = diab_folds,\n            grid = diab_grid, \n            metrics = metric_set(mn_log_loss))\n\n# Now we can take the grid and collect our metrics to see how well each model did\nfits |> collect_metrics() |>\n  filter(.metric == \"mn_log_loss\") |>\n  arrange(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             <dbl>      <int> <chr>       <chr>      <dbl> <int>   <dbl> <chr>  \n 1    0.0000000001          8 mn_log_loss binary     0.333     5 0.00116 Prepro…\n 2    0.000000001           8 mn_log_loss binary     0.333     5 0.00116 Prepro…\n 3    0.00000001            8 mn_log_loss binary     0.333     5 0.00116 Prepro…\n 4    0.0000001             8 mn_log_loss binary     0.333     5 0.00116 Prepro…\n 5    0.000001              8 mn_log_loss binary     0.333     5 0.00116 Prepro…\n 6    0.00001               8 mn_log_loss binary     0.333     5 0.00116 Prepro…\n 7    0.0000000001         11 mn_log_loss binary     0.334     5 0.00237 Prepro…\n 8    0.000000001          11 mn_log_loss binary     0.334     5 0.00237 Prepro…\n 9    0.00000001           11 mn_log_loss binary     0.334     5 0.00237 Prepro…\n10    0.0000001            11 mn_log_loss binary     0.334     5 0.00237 Prepro…\n# ℹ 40 more rows\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now we can extract the best performing of the models\nbest_param <- select_best(fits, metric = \"mn_log_loss\")\nbest_param\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            <dbl>      <int> <chr>                \n1    0.0000000001          8 Preprocessor1_Model21\n```\n\n\n:::\n\n```{.r .cell-code}\n#finalize the workflow which uses the best metric on the model we have\nfinal_wkf <- diab_wkf |> finalize_workflow(best_param)\n```\n:::\n\n\nLooks like Model 31 gave the best performing model for the classification tree method.\n\n## Random Forest\n\nA random forest model uses bootstrapping to create multiple trees from one sample. Each tree is created from a bootstrapped sample taken from the training dataset, done with replacement. The final prediction at the end takes all the trees and averages their results together. This model does not use every predictor at each split of each tree, it considers the splits using a random subset of the predictors each time. This creates a \"random forest\" of decision trees, that are not highly correlated.\n\nWe use random forest models because they are less likely to overfit the data than the regular classification tree method. Additionally, they also have their own built in variable selection, which is helpful when it comes to model selection. It is a robust method of model fitting, due to its choosing different predictors for each tree, and each tree is trained on a different bootstrapped subset.\n\nSet up random forest model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec <- rand_forest(mtry = tune(), trees = 100) |>\nset_engine(\"ranger\", importance = \"impurity\") |>\nset_mode(\"classification\")\n\n# create workflow, using same recipe as the Classification Tree\nrf_wkf <- workflow() |>\nadd_recipe(diab_class) |>\nadd_model(rf_spec)\n\n#using CV for tuned values\nrf_fit <- rf_wkf |> \n  tune_grid(resamples = diab_folds, metrics = metric_set(mn_log_loss))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n\n\n:::\n\n```{.r .cell-code}\n# Looking at all metrics among the trees\nrf_fit |> collect_metrics() |>\nfilter(.metric == \"mn_log_loss\") |> arrange(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  <int> <chr>       <chr>      <dbl> <int>    <dbl> <chr>               \n1     3 mn_log_loss binary     0.324     5 0.000716 Preprocessor1_Model2\n2     2 mn_log_loss binary     0.325     5 0.000870 Preprocessor1_Model3\n3     4 mn_log_loss binary     0.327     5 0.000655 Preprocessor1_Model6\n4     5 mn_log_loss binary     0.331     5 0.000586 Preprocessor1_Model5\n5     6 mn_log_loss binary     0.337     5 0.000589 Preprocessor1_Model7\n6     1 mn_log_loss binary     0.341     5 0.000961 Preprocessor1_Model1\n7     7 mn_log_loss binary     0.383     5 0.00444  Preprocessor1_Model4\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now we can extract the best performing of the models\nrf_best_param <- select_best(rf_fit, metric = \"mn_log_loss\")\nrf_best_param\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n   mtry .config             \n  <int> <chr>               \n1     3 Preprocessor1_Model2\n```\n\n\n:::\n\n```{.r .cell-code}\n#finalize the workflow which uses the best metric on the model we have\nrf_final_wkf <- rf_wkf |> finalize_workflow(rf_best_param)\n```\n:::\n\n\nIt looks like Model 4, which sampled from 3 predictors\n\n## Final Model Selection\n\nHere, we will be determining which of the 3 types of models is the best performing on the test data set overall. We will do this by taking the best model found from each, and comparing them to each other using log loss\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Logistic Regression\nLR2_wkf |> last_fit(split_data, \nmetrics = metric_set(mn_log_loss)) |> collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 mn_log_loss binary         0.337 Preprocessor1_Model1\n```\n\n\n:::\n\n```{.r .cell-code}\n#Classification Tree\nclass_tree_best <- final_wkf |> \nlast_fit(split_data, metrics = metric_set(mn_log_loss)) |>\n  collect_metrics()\nclass_tree_best\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 mn_log_loss binary         0.335 Preprocessor1_Model1\n```\n\n\n:::\n\n```{.r .cell-code}\n#Random Forest\nrf_final_fit <- rf_final_wkf |>\nlast_fit(split_data, metrics = metric_set(mn_log_loss)) |>\n  collect_metrics()\nrf_final_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 mn_log_loss binary         0.325 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\nSo, based on the values of our log loss metric, the Random Forest model was the best performing of the three.\n\nlastly we can extract which variables were used and which were most important\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_full_fit <- rf_final_wkf |> fit(train)\nrf_final_model <- extract_fit_engine(rf_full_fit)\nrf_final_model$variable.importance |> sort()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSmoking_status_yes              Sex_M        HeartDA_yes        Chol_lvl_no \n          94.59461           94.92832          378.38261          834.09820 \n               BMI          BP_lvl_no       Health_level \n        1733.53981         2236.03665         2509.14630 \n```\n\n\n:::\n:::\n\n\nBased on this output, it looks like our most valuable 3 predictors are Health Level, BP_lvl, and BMI\n\nLastly, we must save this model so it can be referenced later in the API. Fitting model to entore data set\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_model <- rf_final_wkf |> fit(diabetes_data_LR)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(final_model, \"final_model.rds\")\n```\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}